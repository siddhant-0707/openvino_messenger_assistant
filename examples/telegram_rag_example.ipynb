{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Telegram RAG System using OpenVINO GenAI and LangChain\n",
    "\n",
    "**Retrieval-augmented generation (RAG)** is a technique for augmenting LLM knowledge with additional, often private or real-time, data. In this notebook, we'll build a RAG system specifically for processing and querying Telegram channel messages.\n",
    "\n",
    "This notebook builds on the concepts from `llm-rag-langchain-genai.ipynb` but focuses on Telegram data sources.\n",
    "\n",
    "The tutorial consists of the following steps:\n",
    "\n",
    "- Install prerequisites\n",
    "- Configure Telegram API access\n",
    "- Select and convert models from Hugging Face (LLM, embedding, reranking)\n",
    "- Compress model weights (INT4/INT8) using NNCF\n",
    "- Download and process Telegram messages\n",
    "- Create a RAG chain pipeline with OpenVINO GenAI\n",
    "- Query and interact with Telegram data\n",
    "\n",
    "The RAG pipeline for Telegram will consist of the following components:\n",
    "\n",
    "1. **Data Ingestion**: Download messages from Telegram channels\n",
    "2. **Article Processing**: Extract and process article content from URLs in messages\n",
    "3. **Document Processing**: Split content into chunks for embedding\n",
    "4. **Vector Store Creation**: Create searchable embeddings with OpenVINO-optimized models\n",
    "5. **Retrieval**: Query relevant content when needed\n",
    "6. **Response Generation**: Generate answers using an LLM with retrieved context\n",
    "\n",
    "### Installation Instructions\n",
    "\n",
    "This is a self-contained example that relies solely on its own code.\n",
    "\n",
    "We recommend running the notebook in a virtual environment. You only need a Jupyter server to start.\n",
    "For details, please refer to [Installation Guide](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/README.md#-installation-guide).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Let's install all the required dependencies for our Telegram RAG system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://storage.openvinotoolkit.org/simple/wheels/nightly\n",
      "Requirement already satisfied: openvino>=2024.2.0 in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (2025.3.0.dev20250707)\n",
      "Collecting openvino>=2024.2.0\n",
      "  Downloading https://storage.openvinotoolkit.org/wheels/nightly/openvino/openvino-2025.3.0.dev20250709-19462-cp312-cp312-manylinux2014_x86_64.whl (47.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.9/47.9 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.3.0,>=1.16.6 in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from openvino>=2024.2.0) (2.2.6)\n",
      "Requirement already satisfied: openvino-telemetry>=2023.2.1 in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from openvino>=2024.2.0) (2025.1.0)\n",
      "Requirement already satisfied: packaging in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from openvino>=2024.2.0) (24.2)\n",
      "Installing collected packages: openvino\n",
      "  Attempting uninstall: openvino\n",
      "    Found existing installation: openvino 2025.3.0.dev20250707\n",
      "    Uninstalling openvino-2025.3.0.dev20250707:\n",
      "      Successfully uninstalled openvino-2025.3.0.dev20250707\n",
      "Successfully installed openvino-2025.3.0.dev20250709\n",
      "Looking in indexes: https://pypi.org/simple, https://storage.openvinotoolkit.org/simple/wheels/nightly\n",
      "Requirement already satisfied: openvino-tokenizers[transformers] in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (2025.3.0.0.dev20250707)\n",
      "Collecting openvino-tokenizers[transformers]\n",
      "  Downloading https://storage.openvinotoolkit.org/wheels/nightly/openvino-tokenizers/openvino_tokenizers-2025.3.0.0.dev20250709-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: openvino~=2025.3.0.dev in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from openvino-tokenizers[transformers]) (2025.3.0.dev20250709)\n",
      "Requirement already satisfied: transformers<=4.51.3,>=4.36.0 in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from transformers[sentencepiece]<=4.51.3,>=4.36.0; extra == \"transformers\"->openvino-tokenizers[transformers]) (4.51.3)\n",
      "Requirement already satisfied: tiktoken<=0.9.0,>=0.3.0 in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from openvino-tokenizers[transformers]) (0.9.0)\n",
      "Requirement already satisfied: numpy<2.3.0,>=1.16.6 in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from openvino~=2025.3.0.dev->openvino-tokenizers[transformers]) (2.2.6)\n",
      "Requirement already satisfied: openvino-telemetry>=2023.2.1 in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from openvino~=2025.3.0.dev->openvino-tokenizers[transformers]) (2025.1.0)\n",
      "Requirement already satisfied: packaging in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from openvino~=2025.3.0.dev->openvino-tokenizers[transformers]) (24.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from tiktoken<=0.9.0,>=0.3.0->openvino-tokenizers[transformers]) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from tiktoken<=0.9.0,>=0.3.0->openvino-tokenizers[transformers]) (2.32.4)\n",
      "Requirement already satisfied: filelock in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from transformers<=4.51.3,>=4.36.0->transformers[sentencepiece]<=4.51.3,>=4.36.0; extra == \"transformers\"->openvino-tokenizers[transformers]) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from transformers<=4.51.3,>=4.36.0->transformers[sentencepiece]<=4.51.3,>=4.36.0; extra == \"transformers\"->openvino-tokenizers[transformers]) (0.32.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from transformers<=4.51.3,>=4.36.0->transformers[sentencepiece]<=4.51.3,>=4.36.0; extra == \"transformers\"->openvino-tokenizers[transformers]) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from transformers<=4.51.3,>=4.36.0->transformers[sentencepiece]<=4.51.3,>=4.36.0; extra == \"transformers\"->openvino-tokenizers[transformers]) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from transformers<=4.51.3,>=4.36.0->transformers[sentencepiece]<=4.51.3,>=4.36.0; extra == \"transformers\"->openvino-tokenizers[transformers]) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from transformers<=4.51.3,>=4.36.0->transformers[sentencepiece]<=4.51.3,>=4.36.0; extra == \"transformers\"->openvino-tokenizers[transformers]) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers<=4.51.3,>=4.36.0->transformers[sentencepiece]<=4.51.3,>=4.36.0; extra == \"transformers\"->openvino-tokenizers[transformers]) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers<=4.51.3,>=4.36.0->transformers[sentencepiece]<=4.51.3,>=4.36.0; extra == \"transformers\"->openvino-tokenizers[transformers]) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers<=4.51.3,>=4.36.0->transformers[sentencepiece]<=4.51.3,>=4.36.0; extra == \"transformers\"->openvino-tokenizers[transformers]) (1.1.3)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from transformers[sentencepiece]<=4.51.3,>=4.36.0; extra == \"transformers\"->openvino-tokenizers[transformers]) (0.2.0)\n",
      "Requirement already satisfied: protobuf in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from transformers[sentencepiece]<=4.51.3,>=4.36.0; extra == \"transformers\"->openvino-tokenizers[transformers]) (6.31.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken<=0.9.0,>=0.3.0->openvino-tokenizers[transformers]) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken<=0.9.0,>=0.3.0->openvino-tokenizers[transformers]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken<=0.9.0,>=0.3.0->openvino-tokenizers[transformers]) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken<=0.9.0,>=0.3.0->openvino-tokenizers[transformers]) (2025.4.26)\n",
      "Installing collected packages: openvino-tokenizers\n",
      "  Attempting uninstall: openvino-tokenizers\n",
      "    Found existing installation: openvino-tokenizers 2025.3.0.0.dev20250707\n",
      "    Uninstalling openvino-tokenizers-2025.3.0.0.dev20250707:\n",
      "      Successfully uninstalled openvino-tokenizers-2025.3.0.0.dev20250707\n",
      "Successfully installed openvino-tokenizers-2025.3.0.0.dev20250709\n",
      "Looking in indexes: https://pypi.org/simple, https://storage.openvinotoolkit.org/simple/wheels/nightly\n",
      "Requirement already satisfied: openvino-genai in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (2025.3.0.0.dev20250707)\n",
      "Collecting openvino-genai\n",
      "  Downloading https://storage.openvinotoolkit.org/wheels/nightly/openvino-genai/openvino_genai-2025.3.0.0.dev20250709-cp312-cp312-manylinux2014_x86_64.whl (4.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: openvino_tokenizers~=2025.3.0.0.dev in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from openvino-genai) (2025.3.0.0.dev20250709)\n",
      "Requirement already satisfied: openvino~=2025.3.0.dev in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from openvino_tokenizers~=2025.3.0.0.dev->openvino-genai) (2025.3.0.dev20250709)\n",
      "Requirement already satisfied: numpy<2.3.0,>=1.16.6 in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from openvino~=2025.3.0.dev->openvino_tokenizers~=2025.3.0.0.dev->openvino-genai) (2.2.6)\n",
      "Requirement already satisfied: openvino-telemetry>=2023.2.1 in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from openvino~=2025.3.0.dev->openvino_tokenizers~=2025.3.0.0.dev->openvino-genai) (2025.1.0)\n",
      "Requirement already satisfied: packaging in /home/sidd/Documents/GitHub/openvino_env/lib/python3.12/site-packages (from openvino~=2025.3.0.dev->openvino_tokenizers~=2025.3.0.0.dev->openvino-genai) (24.2)\n",
      "Installing collected packages: openvino-genai\n",
      "  Attempting uninstall: openvino-genai\n",
      "    Found existing installation: openvino-genai 2025.3.0.0.dev20250707\n",
      "    Uninstalling openvino-genai-2025.3.0.0.dev20250707:\n",
      "      Successfully uninstalled openvino-genai-2025.3.0.0.dev20250707\n",
      "Successfully installed openvino-genai-2025.3.0.0.dev20250709\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# First download utility scripts if needed\n",
    "if not Path(\"notebook_utils.py\").exists():\n",
    "    r = requests.get(\n",
    "        url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    "    )\n",
    "    with open(\"notebook_utils.py\", \"w\") as f:\n",
    "        f.write(r.text)\n",
    "\n",
    "if not Path(\"pip_helper.py\").exists():\n",
    "    r = requests.get(\n",
    "        url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/pip_helper.py\",\n",
    "    )\n",
    "    open(\"pip_helper.py\", \"w\").write(r.text)\n",
    "\n",
    "if not Path(\"cmd_helper.py\").exists():\n",
    "    r = requests.get(\n",
    "        url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/cmd_helper.py\",\n",
    "    )\n",
    "    with open(\"cmd_helper.py\", \"w\") as f:\n",
    "        f.write(r.text)\n",
    "\n",
    "if not Path(\"genai_helper.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/genai_helper.py\")\n",
    "    open(\"genai_helper.py\", \"w\").write(r.text)\n",
    "\n",
    "# Import pip helper to install dependencies\n",
    "from pip_helper import pip_install\n",
    "\n",
    "os.environ[\"GIT_CLONE_PROTECTION_ACTIVE\"] = \"false\"\n",
    "\n",
    "# Install OpenVINO and related packages\n",
    "pip_install(\"--pre\", \"-U\", \"openvino>=2024.2.0\", \"--extra-index-url\", \"https://storage.openvinotoolkit.org/simple/wheels/nightly\")\n",
    "pip_install(\"--pre\", \"-U\", \"openvino-tokenizers[transformers]\", \"--extra-index-url\", \"https://storage.openvinotoolkit.org/simple/wheels/nightly\")\n",
    "pip_install(\"--pre\", \"-U\", \"openvino-genai\", \"--extra-index-url\", \"https://storage.openvinotoolkit.org/simple/wheels/nightly\")\n",
    "\n",
    "# Install other required packages\n",
    "pip_install(\n",
    "    \"-q\",\n",
    "    \"--extra-index-url\",\n",
    "    \"https://download.pytorch.org/whl/cpu\",\n",
    "    \"git+https://github.com/huggingface/optimum-intel.git\",\n",
    "    \"git+https://github.com/openvinotoolkit/nncf.git\",\n",
    "    \"datasets\",\n",
    "    \"accelerate\",\n",
    "    \"gradio>=4.19\",\n",
    "    \"onnx<1.16.2\",\n",
    "    \"einops\",\n",
    "    \"transformers_stream_generator\",\n",
    "    \"tiktoken\",\n",
    "    \"transformers>=4.43.1\",\n",
    "    \"faiss-cpu\",\n",
    "    \"sentence_transformers\",\n",
    "    \"langchain>=0.2.0\",\n",
    "    \"langchain-community>=0.2.15\",\n",
    "    \"langchainhub\",\n",
    "    \"unstructured\",\n",
    "    \"scikit-learn\",\n",
    "    \"python-dotenv\",\n",
    "    \"telethon\",\n",
    "    \"newspaper3k\",\n",
    "    \"beautifulsoup4\",\n",
    "    \"huggingface-hub>=0.26.5\",\n",
    ")\n",
    "\n",
    "# Read more about telemetry collection at https://github.com/openvinotoolkit/openvino_notebooks?tab=readme-ov-file#-telemetry\n",
    "from notebook_utils import collect_telemetry\n",
    "\n",
    "collect_telemetry(\"telegram_rag_example.ipynb\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Model Configuration\n",
    "\n",
    "To make our notebook more configurable, let's fetch the model configuration from `llm_config.py` that contains supported model configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import shutil\n",
    "import io\n",
    "\n",
    "# fetch model configuration\n",
    "config_shared_path = Path(\"../../utils/llm_config.py\")\n",
    "config_dst_path = Path(\"llm_config.py\")\n",
    "\n",
    "if not config_dst_path.exists():\n",
    "    if config_shared_path.exists():\n",
    "        try:\n",
    "            os.symlink(config_shared_path, config_dst_path)\n",
    "        except Exception:\n",
    "            shutil.copy(config_shared_path, config_dst_path)\n",
    "    else:\n",
    "        r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/llm_config.py\")\n",
    "        with open(\"llm_config.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(r.text)\n",
    "elif not os.path.islink(config_dst_path):\n",
    "    print(\"LLM config will be updated\")\n",
    "    if config_shared_path.exists():\n",
    "        shutil.copy(config_shared_path, config_dst_path)\n",
    "    else:\n",
    "        r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/llm_config.py\")\n",
    "        with open(\"llm_config.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(r.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Telegram API Credentials\n",
    "\n",
    "To access Telegram, you need to create API credentials by following these steps:\n",
    "\n",
    "1. Visit https://my.telegram.org/apps\n",
    "2. Log in with your Telegram account\n",
    "3. Create a new application\n",
    "4. Note down your `api_id` and `api_hash`\n",
    "5. Create a `.env` file in the current directory with the following content:\n",
    "\n",
    "```\n",
    "TELEGRAM_API_ID=your_api_id\n",
    "TELEGRAM_API_HASH=your_api_hash\n",
    "```\n",
    "\n",
    "Let's now check if you've set up your credentials correctly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Telegram API credentials loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "api_id = os.getenv(\"TELEGRAM_API_ID\")\n",
    "api_hash = os.getenv(\"TELEGRAM_API_HASH\")\n",
    "\n",
    "if not api_id or not api_hash:\n",
    "    print(\"⚠️ Telegram API credentials not found!\")\n",
    "    print(\"Please create a .env file with your TELEGRAM_API_ID and TELEGRAM_API_HASH\")\n",
    "else:\n",
    "    print(\"✅ Telegram API credentials loaded successfully!\")\n",
    "    \n",
    "# Setup directories\n",
    "telegram_data_dir = Path(\"telegram_data\")\n",
    "telegram_data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "vector_store_path = Path(\"telegram_vector_store\")\n",
    "vector_store_path.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Let's select our models for the RAG pipeline, similar to how it's done in the original notebook:\n",
    "\n",
    "1. The LLM model for generating responses\n",
    "2. The embedding model for creating vector representations\n",
    "3. The reranking model for improving retrieval quality\n",
    "\n",
    "We'll use UI widgets to make this interactive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d0094c82ef430e9bba6d1483d98cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Language:', options=('English', 'Chinese', 'Japanese'), value='English')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llm_config import (\n",
    "    SUPPORTED_EMBEDDING_MODELS,\n",
    "    SUPPORTED_RERANK_MODELS,\n",
    "    SUPPORTED_LLM_MODELS,\n",
    ")\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Widget for model language selection\n",
    "model_languages = list(SUPPORTED_LLM_MODELS)\n",
    "model_language = widgets.Dropdown(\n",
    "    options=model_languages,\n",
    "    value=model_languages[0],\n",
    "    description=\"Language:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "display(model_language)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f94af30883c485da0457c500af678f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='LLM Model:', index=19, options=('tiny-llama-1b-chat', 'llama-3.2-1b-instruct', 'llama-3.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc26629d278434981402e6dd150832b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Embedding:', options=('bge-small-en-v1.5', 'bge-large-en-v1.5', 'bge-m3'), value='bge-sm…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bbc520eae7643d08afeab964133e523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Reranker:', options=('bge-reranker-v2-m3', 'bge-reranker-large', 'bge-reranker-base'), v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select LLM model\n",
    "llm_model_ids = [model_id for model_id, model_config in SUPPORTED_LLM_MODELS[model_language.value].items() if model_config.get(\"rag_prompt_template\")]\n",
    "\n",
    "llm_model_id = widgets.Dropdown(\n",
    "    options=llm_model_ids,\n",
    "    value=llm_model_ids[-1] if llm_model_ids else None,  # Default to last (usually most capable) model\n",
    "    description=\"LLM Model:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "display(llm_model_id)\n",
    "\n",
    "# Select embedding model based on language\n",
    "embedding_model_ids = list(SUPPORTED_EMBEDDING_MODELS[model_language.value])\n",
    "embedding_model_id = widgets.Dropdown(\n",
    "    options=embedding_model_ids,\n",
    "    value=embedding_model_ids[0] if embedding_model_ids else None,\n",
    "    description=\"Embedding:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "display(embedding_model_id)\n",
    "\n",
    "# Select reranking model\n",
    "rerank_model_ids = list(SUPPORTED_RERANK_MODELS)\n",
    "rerank_model_id = widgets.Dropdown(\n",
    "    options=rerank_model_ids,\n",
    "    value=rerank_model_ids[0] if rerank_model_ids else None,\n",
    "    description=\"Reranker:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "display(rerank_model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected LLM: qwen2.5-3b-instruct\n",
      "Selected Embedding model: bge-small-en-v1.5\n",
      "Selected Rerank model: bge-reranker-v2-m3\n"
     ]
    }
   ],
   "source": [
    "# Get the configurations for the selected models\n",
    "llm_model_configuration = SUPPORTED_LLM_MODELS[model_language.value][llm_model_id.value]\n",
    "embedding_model_configuration = SUPPORTED_EMBEDDING_MODELS[model_language.value][embedding_model_id.value]\n",
    "rerank_model_configuration = SUPPORTED_RERANK_MODELS[rerank_model_id.value]\n",
    "\n",
    "print(f\"Selected LLM: {llm_model_id.value}\")\n",
    "print(f\"Selected Embedding model: {embedding_model_id.value}\")\n",
    "print(f\"Selected Rerank model: {rerank_model_id.value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Conversion and Weight Compression\n",
    "\n",
    "Now we'll download, convert, and potentially compress the selected models. \n",
    "\n",
    "1. First, we'll convert the embedding model\n",
    "2. Then the reranking model\n",
    "3. Finally, the LLM with optional weight compression (INT4, INT8, FP16)\n",
    "\n",
    "This process may take some time depending on your internet connection and the size of the selected models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12aae0dd2ed24aff92c0d32da2d5a6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Prepare INT4 model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a11dbbb937d418da6051031cfefa3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Prepare INT8 model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3309ac1a048743e78f1070f52e2eb69c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Prepare FP16 model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec0cb750cbc407bb6d84f252fc91147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Enable AWQ (better quality INT4 compression)')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Options for model weight compression\n",
    "prepare_int4_model = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description=\"Prepare INT4 model\",\n",
    "    disabled=False,\n",
    ")\n",
    "prepare_int8_model = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description=\"Prepare INT8 model\",\n",
    "    disabled=False,\n",
    ")\n",
    "prepare_fp16_model = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description=\"Prepare FP16 model\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "display(prepare_int4_model)\n",
    "display(prepare_int8_model)\n",
    "display(prepare_fp16_model)\n",
    "\n",
    "# AWQ option for INT4 compression\n",
    "enable_awq = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description=\"Enable AWQ (better quality INT4 compression)\",\n",
    "    disabled=not prepare_int4_model.value,\n",
    ")\n",
    "display(enable_awq)\n",
    "\n",
    "# Update AWQ checkbox state when INT4 checkbox changes\n",
    "def update_awq_state(change):\n",
    "    enable_awq.disabled = not change[\"new\"]\n",
    "\n",
    "prepare_int4_model.observe(update_awq_state, names=\"value\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Embedding Model\n",
    "\n",
    "Let's convert the embedding model using Optimum-CLI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model already exists at bge-small-en-v1.5\n"
     ]
    }
   ],
   "source": [
    "from cmd_helper import optimum_cli\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "embedding_model_dir = Path(embedding_model_id.value)\n",
    "\n",
    "if not embedding_model_dir.exists():\n",
    "    print(f\"Converting embedding model {embedding_model_configuration['model_id']}...\")\n",
    "    export_command = f\"optimum-cli export openvino --model {embedding_model_configuration['model_id']} --task feature-extraction {embedding_model_id.value}\"\n",
    "    display(Markdown(f\"**Export command:**\\n```\\n{export_command}\\n```\"))\n",
    "    optimum_cli(\n",
    "        embedding_model_configuration['model_id'], \n",
    "        str(embedding_model_dir), \n",
    "        show_command=False, \n",
    "        additional_args={\"task\": \"feature-extraction\"}\n",
    "    )\n",
    "    print(f\"Embedding model converted and saved to {embedding_model_dir}\")\n",
    "else:\n",
    "    print(f\"Embedding model already exists at {embedding_model_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Reranking Model\n",
    "\n",
    "Now let's convert the reranking model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranking model already exists at bge-reranker-v2-m3\n"
     ]
    }
   ],
   "source": [
    "rerank_model_dir = Path(rerank_model_id.value)\n",
    "\n",
    "if not rerank_model_dir.exists():\n",
    "    print(f\"Converting reranking model {rerank_model_configuration['model_id']}...\")\n",
    "    export_command = f\"optimum-cli export openvino --model {rerank_model_configuration['model_id']} --task text-classification {rerank_model_id.value}\"\n",
    "    display(Markdown(f\"**Export command:**\\n```\\n{export_command}\\n```\"))\n",
    "    optimum_cli(\n",
    "        rerank_model_configuration['model_id'], \n",
    "        str(rerank_model_dir), \n",
    "        show_command=False, \n",
    "        additional_args={\"task\": \"text-classification\"}\n",
    "    )\n",
    "    print(f\"Reranking model converted and saved to {rerank_model_dir}\")\n",
    "else:\n",
    "    print(f\"Reranking model already exists at {rerank_model_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert LLM with Optional Weight Compression\n",
    "\n",
    "Now we'll convert the LLM and potentially apply weight compression (INT4/INT8/FP16).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INT4 model already exists at qwen2.5-3b-instruct/INT4_compressed_weights\n",
      "\n",
      "Comparing model sizes:\n",
      "INT4 model size: 1662.28 MB\n"
     ]
    }
   ],
   "source": [
    "pt_model_id = llm_model_configuration[\"model_id\"]\n",
    "llm_base_dir = Path(llm_model_id.value)\n",
    "fp16_model_dir = llm_base_dir / \"FP16\"\n",
    "int8_model_dir = llm_base_dir / \"INT8_compressed_weights\"\n",
    "int4_model_dir = llm_base_dir / \"INT4_compressed_weights\"\n",
    "\n",
    "# Function to convert to FP16\n",
    "def convert_to_fp16():\n",
    "    if (fp16_model_dir / \"openvino_model.xml\").exists():\n",
    "        print(f\"FP16 model already exists at {fp16_model_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Converting {pt_model_id} to FP16...\")\n",
    "    fp16_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    remote_code = llm_model_configuration.get(\"remote_code\", False)\n",
    "    additional_args = {\"task\": \"text-generation-with-past\", \"weight-format\": \"fp16\"}\n",
    "    if remote_code:\n",
    "        additional_args[\"trust-remote-code\"] = \"\"\n",
    "        \n",
    "    export_command = f\"optimum-cli export openvino --model {pt_model_id} --task text-generation-with-past --weight-format fp16\"\n",
    "    if remote_code:\n",
    "        export_command += \" --trust-remote-code\"\n",
    "    export_command += f\" {fp16_model_dir}\"\n",
    "    \n",
    "    display(Markdown(f\"**Export command:**\\n```\\n{export_command}\\n```\"))\n",
    "    optimum_cli(pt_model_id, fp16_model_dir, show_command=False, additional_args=additional_args)\n",
    "    print(f\"FP16 model saved to {fp16_model_dir}\")\n",
    "\n",
    "# Function to convert to INT8\n",
    "def convert_to_int8():\n",
    "    if (int8_model_dir / \"openvino_model.xml\").exists():\n",
    "        print(f\"INT8 model already exists at {int8_model_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Converting {pt_model_id} to INT8...\")\n",
    "    int8_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    remote_code = llm_model_configuration.get(\"remote_code\", False)\n",
    "    additional_args = {\"task\": \"text-generation-with-past\", \"weight-format\": \"int8\"}\n",
    "    if remote_code:\n",
    "        additional_args[\"trust-remote-code\"] = \"\"\n",
    "        \n",
    "    export_command = f\"optimum-cli export openvino --model {pt_model_id} --task text-generation-with-past --weight-format int8\"\n",
    "    if remote_code:\n",
    "        export_command += \" --trust-remote-code\"\n",
    "    export_command += f\" {int8_model_dir}\"\n",
    "    \n",
    "    display(Markdown(f\"**Export command:**\\n```\\n{export_command}\\n```\"))\n",
    "    optimum_cli(pt_model_id, int8_model_dir, show_command=False, additional_args=additional_args)\n",
    "    print(f\"INT8 model saved to {int8_model_dir}\")\n",
    "\n",
    "# Function to convert to INT4\n",
    "def convert_to_int4():\n",
    "    if (int4_model_dir / \"openvino_model.xml\").exists():\n",
    "        print(f\"INT4 model already exists at {int4_model_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Converting {pt_model_id} to INT4...\")\n",
    "    int4_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Compression configurations for different models\n",
    "    compression_configs = {\n",
    "        \"qwen2.5-7b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "        \"qwen2.5-3b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "        \"qwen2.5-14b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "        \"qwen2.5-1.5b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "        \"qwen2.5-0.5b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "        \"default\": {\"sym\": False, \"group_size\": 128, \"ratio\": 0.8},\n",
    "    }\n",
    "    \n",
    "    model_compression_params = compression_configs.get(llm_model_id.value, compression_configs[\"default\"])\n",
    "    remote_code = llm_model_configuration.get(\"remote_code\", False)\n",
    "    \n",
    "    additional_args = {\n",
    "        \"task\": \"text-generation-with-past\",\n",
    "        \"weight-format\": \"int4\", \n",
    "        \"group-size\": model_compression_params[\"group_size\"], \n",
    "        \"ratio\": model_compression_params[\"ratio\"]\n",
    "    }\n",
    "    \n",
    "    export_command = f\"optimum-cli export openvino --model {pt_model_id} --task text-generation-with-past --weight-format int4\"\n",
    "    export_command += f\" --group-size {model_compression_params['group_size']} --ratio {model_compression_params['ratio']}\"\n",
    "    \n",
    "    if model_compression_params[\"sym\"]:\n",
    "        export_command += \" --sym\"\n",
    "        additional_args[\"sym\"] = \"\"\n",
    "        \n",
    "    if enable_awq.value:\n",
    "        export_command += \" --awq --dataset wikitext2 --num-samples 128\"\n",
    "        additional_args.update({\"dataset\": \"wikitext2\", \"awq\": \"\", \"num-samples\": \"128\"})\n",
    "        \n",
    "    if remote_code:\n",
    "        export_command += \" --trust-remote-code\"\n",
    "        additional_args[\"trust-remote-code\"] = \"\"\n",
    "        \n",
    "    export_command += f\" {int4_model_dir}\"\n",
    "    \n",
    "    display(Markdown(f\"**Export command:**\\n```\\n{export_command}\\n```\"))\n",
    "    optimum_cli(pt_model_id, int4_model_dir, show_command=False, additional_args=additional_args)\n",
    "    print(f\"INT4 model saved to {int4_model_dir}\")\n",
    "\n",
    "# Execute the selected conversions\n",
    "if prepare_fp16_model.value:\n",
    "    convert_to_fp16()\n",
    "if prepare_int8_model.value:\n",
    "    convert_to_int8()\n",
    "if prepare_int4_model.value:\n",
    "    convert_to_int4()\n",
    "    \n",
    "# Compare model sizes\n",
    "print(\"\\nComparing model sizes:\")\n",
    "if os.path.exists(fp16_model_dir / \"openvino_model.bin\"):\n",
    "    fp16_size = (fp16_model_dir / \"openvino_model.bin\").stat().st_size / (1024 * 1024)\n",
    "    print(f\"FP16 model size: {fp16_size:.2f} MB\")\n",
    "\n",
    "if os.path.exists(int8_model_dir / \"openvino_model.bin\"):\n",
    "    int8_size = (int8_model_dir / \"openvino_model.bin\").stat().st_size / (1024 * 1024)\n",
    "    print(f\"INT8 model size: {int8_size:.2f} MB\")\n",
    "    if os.path.exists(fp16_model_dir / \"openvino_model.bin\"):\n",
    "        print(f\"INT8 compression ratio: {fp16_size / int8_size:.2f}x\")\n",
    "\n",
    "if os.path.exists(int4_model_dir / \"openvino_model.bin\"):\n",
    "    int4_size = (int4_model_dir / \"openvino_model.bin\").stat().st_size / (1024 * 1024)\n",
    "    print(f\"INT4 model size: {int4_size:.2f} MB\")\n",
    "    if os.path.exists(fp16_model_dir / \"openvino_model.bin\"):\n",
    "        print(f\"INT4 compression ratio: {fp16_size / int4_size:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Selection for Models\n",
    "\n",
    "Now we'll select the devices to use for each model in our pipeline:\n",
    "1. The device for the embedding model\n",
    "2. The device for the reranking model \n",
    "3. The device for the LLM\n",
    "\n",
    "Each model can be run on CPU, GPU, or set to AUTO (to automatically select the most suitable device).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select device for embedding model:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95cb873a699f4a55a5d9ff679b57ad40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=2, options=('CPU', 'GPU', 'AUTO'), value='AUTO')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select device for reranking model:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a182dfa63fc4cc987018175bdae205e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=2, options=('CPU', 'GPU', 'AUTO'), value='AUTO')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select device for LLM:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7415822fa7646aabb84828c58ceac8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=2, options=('CPU', 'GPU', 'AUTO'), value='AUTO')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding model will be loaded to AUTO device\n",
      "Reranking model will be loaded to AUTO device\n",
      "LLM model will be loaded to AUTO device\n"
     ]
    }
   ],
   "source": [
    "from notebook_utils import device_widget\n",
    "\n",
    "# Device for embedding model\n",
    "print(\"Select device for embedding model:\")\n",
    "embedding_device = device_widget()\n",
    "display(embedding_device)\n",
    "\n",
    "# Device for reranking model\n",
    "print(\"Select device for reranking model:\")\n",
    "rerank_device = device_widget()\n",
    "display(rerank_device)\n",
    "\n",
    "# Device for LLM\n",
    "print(\"Select device for LLM:\")\n",
    "llm_device = device_widget()\n",
    "display(llm_device)\n",
    "\n",
    "# Print the selected devices\n",
    "print(f\"\\nEmbedding model will be loaded to {embedding_device.value} device\")\n",
    "print(f\"Reranking model will be loaded to {rerank_device.value} device\")\n",
    "print(f\"LLM model will be loaded to {llm_device.value} device\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select LLM Model Variant\n",
    "\n",
    "Now let's choose which variant of the LLM model to use (INT4, INT8, or FP16):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53c849d0d1342f59ed4ea0891192685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model to use:', options=('INT4',), value='INT4')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected INT4 model at qwen2.5-3b-instruct/INT4_compressed_weights\n"
     ]
    }
   ],
   "source": [
    "# Determine which model variants are available\n",
    "available_models = []\n",
    "if os.path.exists(int4_model_dir / \"openvino_model.xml\"):\n",
    "    available_models.append(\"INT4\")\n",
    "if os.path.exists(int8_model_dir / \"openvino_model.xml\"):\n",
    "    available_models.append(\"INT8\")\n",
    "if os.path.exists(fp16_model_dir / \"openvino_model.xml\"):\n",
    "    available_models.append(\"FP16\")\n",
    "\n",
    "if not available_models:\n",
    "    print(\"No converted models available yet. Please run the model conversion cell first.\")\n",
    "    # Default to INT4 for widget initialization\n",
    "    available_models = [\"INT4\"]\n",
    "\n",
    "model_to_run = widgets.Dropdown(\n",
    "    options=available_models,\n",
    "    value=available_models[0],\n",
    "    description=\"Model to use:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "display(model_to_run)\n",
    "\n",
    "# Determine the actual model directory to use\n",
    "if model_to_run.value == \"INT4\":\n",
    "    model_dir = int4_model_dir\n",
    "elif model_to_run.value == \"INT8\":\n",
    "    model_dir = int8_model_dir\n",
    "else:\n",
    "    model_dir = fp16_model_dir\n",
    "    \n",
    "print(f\"Selected {model_to_run.value} model at {model_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models\n",
    "\n",
    "Now let's load all our models into memory:\n",
    "\n",
    "1. The embedding model\n",
    "2. The reranking model\n",
    "3. The LLM\n",
    "\n",
    "We'll use OpenVINO optimizations for all models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Embedding Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded successfully. Output dimension: 384\n",
      "Sample values: [-0.04453131  0.05994703 -0.06316422]\n"
     ]
    }
   ],
   "source": [
    "from ov_langchain_helper import OpenVINOBgeEmbeddings\n",
    "from notebook_utils import optimize_bge_embedding\n",
    "\n",
    "# Check if we're using NPU and need to optimize the model for it\n",
    "USING_NPU = embedding_device.value == \"NPU\"\n",
    "npu_embedding_dir = Path(str(embedding_model_dir) + \"-npu\")\n",
    "npu_embedding_path = npu_embedding_dir / \"openvino_model.xml\"\n",
    "\n",
    "if USING_NPU and not npu_embedding_dir.exists():\n",
    "    print(\"Optimizing embedding model for NPU...\")\n",
    "    shutil.copytree(embedding_model_dir, npu_embedding_dir)\n",
    "    optimize_bge_embedding(embedding_model_dir / \"openvino_model.xml\", npu_embedding_path)\n",
    "    print(f\"NPU-optimized model saved to {npu_embedding_dir}\")\n",
    "\n",
    "# Prepare embedding model parameters\n",
    "embedding_model_name = str(npu_embedding_dir) if USING_NPU else str(embedding_model_dir)\n",
    "batch_size = 1 if USING_NPU else 4\n",
    "embedding_model_kwargs = {\"device_name\": embedding_device.value}\n",
    "encode_kwargs = {\n",
    "    \"mean_pooling\": embedding_model_configuration[\"mean_pooling\"],\n",
    "    \"normalize_embeddings\": embedding_model_configuration[\"normalize_embeddings\"],\n",
    "    \"batch_size\": batch_size,\n",
    "}\n",
    "\n",
    "# Load the embedding model\n",
    "if USING_NPU:\n",
    "    import openvino as ov\n",
    "    \n",
    "    core = ov.Core()\n",
    "    embedding_model = core.read_model(Path(embedding_model_name) / \"openvino_model.xml\")\n",
    "    port_to_shape = dict()\n",
    "    for input_port in embedding_model.inputs:\n",
    "        port_to_shape[input_port] = [1, 512]\n",
    "    embedding_model.reshape(port_to_shape)\n",
    "    embedding_model = core.compile_model(embedding_model, embedding_device.value)\n",
    "    \n",
    "    embedding = OpenVINOBgeEmbeddings(\n",
    "        ov_model=embedding_model,\n",
    "        model_path=embedding_model_name,\n",
    "        model_kwargs=embedding_model_kwargs,\n",
    "        encode_kwargs=encode_kwargs,\n",
    "    )\n",
    "else:\n",
    "    embedding = OpenVINOBgeEmbeddings(\n",
    "        model_path=embedding_model_name,\n",
    "        model_kwargs=embedding_model_kwargs,\n",
    "        encode_kwargs=encode_kwargs,\n",
    "    )\n",
    "\n",
    "# Test the embedding model\n",
    "test_text = \"This is a test message from Telegram.\"\n",
    "embedding_result = embedding.embed_query(test_text)\n",
    "print(f\"Embedding model loaded successfully. Output dimension: {len(embedding_result)}\")\n",
    "print(f\"Sample values: {embedding_result[:3]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Reranking Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranking model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from ov_langchain_helper import OpenVINOReranker\n",
    "\n",
    "# Configure reranking parameters\n",
    "rerank_model_name = rerank_model_id.value\n",
    "rerank_model_kwargs = {\"device_name\": rerank_device.value}\n",
    "rerank_top_n = 2\n",
    "vector_search_top_k_npu = 4\n",
    "\n",
    "# Load the reranking model\n",
    "if rerank_device.value == \"NPU\":\n",
    "    import openvino as ov\n",
    "\n",
    "    core = ov.Core()\n",
    "    rerank_model = core.read_model(Path(rerank_model_name) / \"openvino_model.xml\")\n",
    "    port_to_shape = dict()\n",
    "    for input_port in rerank_model.inputs:\n",
    "        port_to_shape[input_port] = [vector_search_top_k_npu, 512]\n",
    "    rerank_model.reshape(port_to_shape)\n",
    "\n",
    "    rerank_model = core.compile_model(rerank_model, rerank_device.value)\n",
    "\n",
    "    reranker = OpenVINOReranker(\n",
    "        ov_model=rerank_model,\n",
    "        model_path=rerank_model_name,\n",
    "        top_n=rerank_top_n,\n",
    "    )\n",
    "else:\n",
    "    reranker = OpenVINOReranker(\n",
    "        model_path=rerank_model_name,\n",
    "        model_kwargs=rerank_model_kwargs,\n",
    "        top_n=rerank_top_n,\n",
    "    )\n",
    "    \n",
    "print(\"Reranking model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LLM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM test output: Hello! How may I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from ov_langchain_helper import OpenVINOLLM\n",
    "\n",
    "# Load the LLM\n",
    "llm = OpenVINOLLM.from_model_path(\n",
    "    model_path=str(model_dir),\n",
    "    device=llm_device.value,\n",
    ")\n",
    "\n",
    "# Set default parameters\n",
    "llm.config.max_new_tokens = 1024\n",
    "llm.config.temperature = 0.7\n",
    "llm.config.top_p = 0.9\n",
    "llm.config.top_k = 50\n",
    "llm.config.repetition_penalty = 1.1\n",
    "llm.config.do_sample = True\n",
    "\n",
    "# Test the LLM\n",
    "test_output = llm.invoke(\"Hello, this is a test. Please respond with just one short sentence.\")\n",
    "print(f\"LLM test output: {test_output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Telegram Channel Ingestion\n",
    "\n",
    "Now let's create a component to download messages from Telegram channels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4508d176205340e38da76afea3cc590a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='guardian,bloomberg', description='Channels:', placeholder='Enter channel names (comma-separated)')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b6fae173ed4583a55502128f372da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=100, description='Msg Limit:', max=1000, min=10, step=10, tooltip='Maximum messages per channe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c3094b87cf4686a8c7272fcab6c026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=24, description='Hours:', max=168, min=1, tooltip='Download messages from the last N hours')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47d3db2ecba41d597e881323ea842c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Download Messages', style=ButtonStyle(), tooltip='Click to downloa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5519b0a63140fd9c058988beadc734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "from datetime import datetime, timedelta\n",
    "from telegram_ingestion import TelegramChannelIngestion\n",
    "from article_processor import ArticleProcessor\n",
    "\n",
    "# Function to download messages from Telegram channels\n",
    "async def download_messages(channels, limit_per_channel=100, since_hours=24):\n",
    "    \"\"\"Download messages from specified Telegram channels\"\"\"\n",
    "    \n",
    "    if not api_id or not api_hash:\n",
    "        return \"Error: Telegram API credentials not set. Please configure them in the .env file.\"\n",
    "    \n",
    "    try:\n",
    "        ingestion = TelegramChannelIngestion(\n",
    "            api_id=api_id,\n",
    "            api_hash=api_hash,\n",
    "            storage_dir=str(telegram_data_dir)\n",
    "        )\n",
    "        \n",
    "        await ingestion.start()\n",
    "        try:\n",
    "            messages = await ingestion.process_channels(\n",
    "                channels,\n",
    "                limit_per_channel=limit_per_channel,\n",
    "                since_hours=since_hours\n",
    "            )\n",
    "            \n",
    "            # Process messages to extract article content\n",
    "            article_processor = ArticleProcessor()\n",
    "            processed_messages = article_processor.process_messages(messages)\n",
    "            \n",
    "            return f\"Successfully downloaded and processed {len(processed_messages)} messages from {len(channels)} channels\"\n",
    "        finally:\n",
    "            await ingestion.stop()\n",
    "    except Exception as e:\n",
    "        return f\"Error downloading messages: {str(e)}\"\n",
    "\n",
    "# UI for entering channel names and download parameters\n",
    "channel_names = widgets.Text(\n",
    "    value='guardian,bloomberg',  # Default channels\n",
    "    placeholder='Enter channel names (comma-separated)',\n",
    "    description='Channels:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "limit_slider = widgets.IntSlider(\n",
    "    value=100,\n",
    "    min=10,\n",
    "    max=1000,\n",
    "    step=10,\n",
    "    description='Msg Limit:',\n",
    "    tooltip='Maximum messages per channel',\n",
    ")\n",
    "\n",
    "hours_slider = widgets.IntSlider(\n",
    "    value=24,\n",
    "    min=1,\n",
    "    max=168,  # 1 week\n",
    "    step=1,\n",
    "    description='Hours:',\n",
    "    tooltip='Download messages from the last N hours',\n",
    ")\n",
    "\n",
    "display(channel_names)\n",
    "display(limit_slider)\n",
    "display(hours_slider)\n",
    "\n",
    "# Button to trigger download\n",
    "download_button = widgets.Button(\n",
    "    description='Download Messages',\n",
    "    button_style='primary',\n",
    "    tooltip='Click to download messages from the specified channels'\n",
    ")\n",
    "\n",
    "download_status = widgets.Output()\n",
    "\n",
    "from IPython.display import display\n",
    "import asyncio\n",
    "from functools import partial\n",
    "\n",
    "async def _download_messages_wrapper(channels, limit_per_channel, since_hours):\n",
    "    with download_status:\n",
    "        download_status.clear_output()\n",
    "        if not channels:\n",
    "            print(\"Please enter at least one channel name\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Downloading messages from: {', '.join(channels)}...\")\n",
    "        try:\n",
    "            result = await download_messages(channels, limit_per_channel, since_hours)\n",
    "            print(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "\n",
    "def on_download_button_clicked(b):\n",
    "    channels = [c.strip() for c in channel_names.value.split(',') if c.strip()]\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.create_task(_download_messages_wrapper(channels, limit_slider.value, hours_slider.value))\n",
    "\n",
    "download_button.on_click(on_download_button_clicked)\n",
    "\n",
    "display(download_button)\n",
    "display(download_status)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading vector store: Error in faiss::FileIOReader::FileIOReader(const char*) at /project/faiss/faiss/impl/io.cpp:67: Error: 'f' failed: could not open telegram_vector_store/index.faiss for reading: No such file or directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfdd008f48641fa901323349cb8f0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='info', description='Process Messages', style=ButtonStyle(), tooltip='Process downloaded m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44765798c3b4073b04aa90152f0c0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from telegram_rag_integration import TelegramRAGIntegration\n",
    "import json\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize RAG integration with our OpenVINO-optimized models\n",
    "telegram_rag = TelegramRAGIntegration(\n",
    "    embedding_model_name=str(embedding_model_dir),\n",
    "    vector_store_path=str(vector_store_path),\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "# Function to process downloaded messages into vector store\n",
    "def process_messages_to_vectorstore():\n",
    "    \"\"\"Process all downloaded messages into the vector store\"\"\"\n",
    "    try:\n",
    "        telegram_rag.process_telegram_data_dir(data_dir=str(telegram_data_dir))\n",
    "        return \"Successfully processed messages into vector store\"\n",
    "    except Exception as e:\n",
    "        return f\"Error processing messages: {str(e)}\"\n",
    "\n",
    "# Button to process messages\n",
    "process_button = widgets.Button(\n",
    "    description='Process Messages',\n",
    "    button_style='info',\n",
    "    tooltip='Process downloaded messages into vector store'\n",
    ")\n",
    "\n",
    "process_status = widgets.Output()\n",
    "\n",
    "@process_button.on_click\n",
    "def on_process_button_clicked(b):\n",
    "    with process_status:\n",
    "        process_status.clear_output()\n",
    "        print(\"Processing messages into vector store...\")\n",
    "        result = process_messages_to_vectorstore()\n",
    "        print(result)\n",
    "\n",
    "display(process_button)\n",
    "display(process_status)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Telegram Messages\n",
    "\n",
    "Now let's create a UI for querying the processed Telegram messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bacb33c26434a87b2eeda655ed791d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Search:', placeholder='Enter your search query')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4820cbcdb44ebe9f696e1abb33151b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Channel:', placeholder='Optional: filter by channel name')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7cf021630747f8afe9e166f1c7adf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=5, description='Results:', max=20, min=1, tooltip='Number of results to show')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5934128f773249f1b8863865ad2b278a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Search Messages', style=ButtonStyle(), tooltip='Search for relevan…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86bd645cf36c4c1da3ec1cef6604eccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to query messages\n",
    "def query_messages(query, channel=None, num_results=5):\n",
    "    \"\"\"Query the vector store for relevant messages\"\"\"\n",
    "    try:\n",
    "        filter_dict = {\"channel\": channel} if channel else None\n",
    "        results = telegram_rag.query_messages(query, k=num_results, filter_dict=filter_dict)\n",
    "        \n",
    "        output = []\n",
    "        for i, doc in enumerate(results, 1):\n",
    "            output.append(f\"Result {i}:\")\n",
    "            output.append(f\"Channel: {doc.metadata.get('channel', 'Unknown')}\")\n",
    "            output.append(f\"Date: {doc.metadata.get('date', 'Unknown')}\")\n",
    "            \n",
    "            # Check if this is an article result\n",
    "            if 'article_title' in doc.metadata:\n",
    "                output.append(f\"Article Title: {doc.metadata['article_title']}\")\n",
    "                if 'article_url' in doc.metadata:\n",
    "                    output.append(f\"Article URL: {doc.metadata['article_url']}\")\n",
    "            \n",
    "            # Show content snippet\n",
    "            content = doc.page_content\n",
    "            if len(content) > 300:\n",
    "                content = content[:300] + \"...\"\n",
    "            output.append(f\"Content: {content}\")\n",
    "            output.append(\"\")\n",
    "            \n",
    "        if not output:\n",
    "            return \"No results found\"\n",
    "        return \"\\n\".join(output)\n",
    "    except Exception as e:\n",
    "        return f\"Error querying messages: {str(e)}\"\n",
    "\n",
    "# UI for searching messages\n",
    "query_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter your search query',\n",
    "    description='Search:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "channel_filter = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Optional: filter by channel name',\n",
    "    description='Channel:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "num_results_slider = widgets.IntSlider(\n",
    "    value=5,\n",
    "    min=1,\n",
    "    max=20,\n",
    "    step=1,\n",
    "    description='Results:',\n",
    "    tooltip='Number of results to show',\n",
    ")\n",
    "\n",
    "display(query_input)\n",
    "display(channel_filter)\n",
    "display(num_results_slider)\n",
    "\n",
    "# Button to trigger search\n",
    "search_button = widgets.Button(\n",
    "    description='Search Messages',\n",
    "    button_style='success',\n",
    "    tooltip='Search for relevant messages'\n",
    ")\n",
    "\n",
    "search_results = widgets.Output()\n",
    "\n",
    "@search_button.on_click\n",
    "def on_search_button_clicked(b):\n",
    "    with search_results:\n",
    "        search_results.clear_output()\n",
    "        query = query_input.value.strip()\n",
    "        if not query:\n",
    "            print(\"Please enter a search query\")\n",
    "            return\n",
    "        \n",
    "        channel = channel_filter.value.strip() or None\n",
    "        print(f\"Searching for: '{query}' \" + (f\"in channel: {channel}\" if channel else \"in all channels\"))\n",
    "        result = query_messages(query, channel, num_results_slider.value)\n",
    "        print(result)\n",
    "\n",
    "display(search_button)\n",
    "display(search_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask Questions with RAG\n",
    "\n",
    "Now let's create a UI for asking questions about the Telegram messages using the RAG system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e00760c16a46069ed4b4b0babdb349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Question:', layout=Layout(width='80%'), placeholder='Ask a question about the Tele…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1018b313614606bb7f1ae382d7dedf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Channel:', placeholder='Optional: filter by channel name')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec786736ebf04447bd1ba546f4c70ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.7, description='Temp:', max=1.0, min=0.1, tooltip='Temperature (higher = more creative)')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1975fc6bffc54096ad291def6ecfc31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=5, description='Context:', max=10, min=1, tooltip='Number of messages to use as context')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847166c86ac34092a9bafd958bef8da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Show retrieved context', indent=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d713ce890ec48f78416500355711dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Ask Question', style=ButtonStyle(), tooltip='Ask a question about …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43ceadf777e460880f1792800adc1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# Create a RAG prompt template based on the LLM model's configuration\n",
    "rag_prompt_template = llm_model_configuration[\"rag_prompt_template\"]\n",
    "\n",
    "# Function to answer questions about Telegram messages using RAG\n",
    "def answer_question(question, channel=None, temperature=0.7, top_k=5, show_retrieved=False):\n",
    "    \"\"\"\n",
    "    Answer questions about Telegram messages using RAG\n",
    "    \n",
    "    Args:\n",
    "        question: The question to answer\n",
    "        channel: Optional channel to filter results\n",
    "        temperature: Temperature for LLM generation\n",
    "        top_k: Number of relevant messages to retrieve\n",
    "        show_retrieved: Whether to show the retrieved context\n",
    "        \n",
    "    Returns:\n",
    "        Generated answer and optionally the retrieved context\n",
    "    \"\"\"\n",
    "    try:\n",
    "        filter_dict = {\"channel\": channel} if channel else None\n",
    "        \n",
    "        # Update LLM parameters\n",
    "        llm.config.temperature = temperature\n",
    "        llm.config.top_p = 0.9\n",
    "        llm.config.top_k = 50\n",
    "        llm.config.repetition_penalty = 1.1\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = PromptTemplate.from_template(rag_prompt_template)\n",
    "        \n",
    "        # Create retriever\n",
    "        if rerank_device.value == \"NPU\":\n",
    "            vector_search_top_k = vector_search_top_k_npu\n",
    "        else:\n",
    "            vector_search_top_k = top_k * 2  # Retrieve more for reranking\n",
    "            \n",
    "        retriever = telegram_rag.vectorstore.as_retriever(\n",
    "            search_kwargs={\"k\": vector_search_top_k},\n",
    "            search_type=\"similarity\"\n",
    "        )\n",
    "        \n",
    "        # Add reranking if available\n",
    "        if hasattr(reranker, 'top_n'):\n",
    "            reranker.top_n = top_k\n",
    "            from langchain.retrievers import ContextualCompressionRetriever\n",
    "            retriever = ContextualCompressionRetriever(\n",
    "                base_compressor=reranker,\n",
    "                base_retriever=retriever\n",
    "            )\n",
    "            \n",
    "        # Create RAG chain\n",
    "        combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
    "        rag_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "        \n",
    "        # Run the chain\n",
    "        response = rag_chain.invoke({\"input\": question})\n",
    "        \n",
    "        if show_retrieved and \"context\" in response:\n",
    "            context_docs = []\n",
    "            for i, doc in enumerate(response[\"context\"], 1):\n",
    "                context_docs.append(f\"Document {i}:\")\n",
    "                context_docs.append(f\"Channel: {doc.metadata.get('channel', 'Unknown')}\")\n",
    "                context_docs.append(f\"Date: {doc.metadata.get('date', 'Unknown')}\")\n",
    "                \n",
    "                # Show content snippet\n",
    "                content = doc.page_content\n",
    "                if len(content) > 200:\n",
    "                    content = content[:200] + \"...\"\n",
    "                context_docs.append(f\"Content: {content}\")\n",
    "                context_docs.append(\"\")\n",
    "                \n",
    "            context_str = \"\\n\".join(context_docs)\n",
    "            return f\"{response['answer']}\\n\\n--- Retrieved Context ---\\n{context_str}\"\n",
    "        else:\n",
    "            return response[\"answer\"]\n",
    "            \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        return f\"Error answering question: {str(e)}\\n{traceback.format_exc()}\"\n",
    "\n",
    "# UI for asking questions\n",
    "question_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Ask a question about the Telegram messages',\n",
    "    description='Question:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "qa_channel_filter = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Optional: filter by channel name',\n",
    "    description='Channel:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "temperature_slider = widgets.FloatSlider(\n",
    "    value=0.7,\n",
    "    min=0.1,\n",
    "    max=1.0,\n",
    "    step=0.1,\n",
    "    description='Temp:',\n",
    "    tooltip='Temperature (higher = more creative)',\n",
    ")\n",
    "\n",
    "context_size_slider = widgets.IntSlider(\n",
    "    value=5,\n",
    "    min=1,\n",
    "    max=10,\n",
    "    step=1,\n",
    "    description='Context:',\n",
    "    tooltip='Number of messages to use as context',\n",
    ")\n",
    "\n",
    "show_retrieved_checkbox = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Show retrieved context',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "display(question_input)\n",
    "display(qa_channel_filter)\n",
    "display(temperature_slider)\n",
    "display(context_size_slider)\n",
    "display(show_retrieved_checkbox)\n",
    "\n",
    "# Button to trigger question answering\n",
    "ask_button = widgets.Button(\n",
    "    description='Ask Question',\n",
    "    button_style='primary',\n",
    "    tooltip='Ask a question about the Telegram messages'\n",
    ")\n",
    "\n",
    "qa_results = widgets.Output()\n",
    "\n",
    "@ask_button.on_click\n",
    "def on_ask_button_clicked(b):\n",
    "    with qa_results:\n",
    "        qa_results.clear_output()\n",
    "        question = question_input.value.strip()\n",
    "        if not question:\n",
    "            print(\"Please enter a question\")\n",
    "            return\n",
    "        \n",
    "        channel = qa_channel_filter.value.strip() or None\n",
    "        print(f\"Question: {question}\")\n",
    "        print(\"Generating answer...\")\n",
    "        \n",
    "        result = answer_question(\n",
    "            question,\n",
    "            channel=channel,\n",
    "            temperature=temperature_slider.value,\n",
    "            top_k=context_size_slider.value,\n",
    "            show_retrieved=show_retrieved_checkbox.value\n",
    "        )\n",
    "        \n",
    "        print(\"\\nAnswer:\")\n",
    "        print(result)\n",
    "\n",
    "display(ask_button)\n",
    "display(qa_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio UI (Optional)\n",
    "\n",
    "You can also run a Gradio UI for a more user-friendly interface. Run the following cell to launch the Gradio app.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def launch_gradio_app():\n",
    "    with gr.Blocks(title=\"Telegram RAG System\") as demo:\n",
    "        gr.Markdown(\"# Telegram RAG System with OpenVINO\")\n",
    "        \n",
    "        with gr.Tab(\"Download Messages\"):\n",
    "            gr.Markdown(\"## Download Messages from Telegram Channels\")\n",
    "            channels_input = gr.Textbox(\n",
    "                label=\"Channel Names (comma-separated)\",\n",
    "                placeholder=\"Enter channel names without @ symbol (e.g., guardian, bloomberg)\",\n",
    "                value=\"guardian,bloomberg\"\n",
    "            )\n",
    "            limit_input = gr.Slider(\n",
    "                minimum=1,\n",
    "                maximum=1000,\n",
    "                value=100,\n",
    "                step=1,\n",
    "                label=\"Messages per Channel\"\n",
    "            )\n",
    "            hours_input = gr.Slider(\n",
    "                minimum=1,\n",
    "                maximum=168,\n",
    "                value=24,\n",
    "                step=1,\n",
    "                label=\"Hours to Look Back\"\n",
    "            )\n",
    "            download_btn = gr.Button(\"Download Messages\")\n",
    "            download_status = gr.Textbox(label=\"Download Status\")\n",
    "            \n",
    "            def download_handler(channels_str, limit, hours):\n",
    "                channels = [c.strip() for c in channels_str.split(\",\") if c.strip()]\n",
    "                if not channels:\n",
    "                    return \"Please provide at least one channel name\"\n",
    "                \n",
    "                result = asyncio.run(download_messages(channels, limit, hours))\n",
    "                return result\n",
    "            \n",
    "            download_btn.click(\n",
    "                fn=download_handler,\n",
    "                inputs=[channels_input, limit_input, hours_input],\n",
    "                outputs=[download_status]\n",
    "            )\n",
    "            \n",
    "        with gr.Tab(\"Process Messages\"):\n",
    "            gr.Markdown(\"## Process Downloaded Messages\")\n",
    "            process_btn = gr.Button(\"Process Messages\")\n",
    "            process_output = gr.Textbox(label=\"Processing Status\")\n",
    "            \n",
    "            process_btn.click(\n",
    "                fn=process_messages_to_vectorstore,\n",
    "                inputs=[],\n",
    "                outputs=process_output\n",
    "            )\n",
    "            \n",
    "        with gr.Tab(\"Query Messages\"):\n",
    "            gr.Markdown(\"## Query Processed Messages\")\n",
    "            query_input = gr.Textbox(\n",
    "                label=\"Search Query\",\n",
    "                placeholder=\"Enter your search query\"\n",
    "            )\n",
    "            channel_filter = gr.Textbox(\n",
    "                label=\"Filter by Channel (Optional)\",\n",
    "                placeholder=\"Enter channel name to filter results\"\n",
    "            )\n",
    "            num_results = gr.Slider(\n",
    "                minimum=1,\n",
    "                maximum=20,\n",
    "                value=5,\n",
    "                step=1,\n",
    "                label=\"Number of Results\"\n",
    "            )\n",
    "            query_btn = gr.Button(\"Search\")\n",
    "            query_output = gr.Textbox(label=\"Search Results\", lines=20)\n",
    "            \n",
    "            query_btn.click(\n",
    "                fn=query_messages,\n",
    "                inputs=[query_input, channel_filter, num_results],\n",
    "                outputs=query_output\n",
    "            )\n",
    "        \n",
    "        with gr.Tab(\"Question Answering\"):\n",
    "            gr.Markdown(\"## Ask Questions About Messages\")\n",
    "            question_input = gr.Textbox(\n",
    "                label=\"Question\",\n",
    "                placeholder=\"Ask a question about the Telegram messages\"\n",
    "            )\n",
    "            qa_channel_filter = gr.Textbox(\n",
    "                label=\"Filter by Channel (Optional)\",\n",
    "                placeholder=\"Enter channel name to filter results\"\n",
    "            )\n",
    "            temperature_slider = gr.Slider(\n",
    "                minimum=0.1,\n",
    "                maximum=1.0,\n",
    "                value=0.7,\n",
    "                step=0.1,\n",
    "                label=\"Temperature (controls creativity)\"\n",
    "            )\n",
    "            context_slider = gr.Slider(\n",
    "                minimum=1,\n",
    "                maximum=10,\n",
    "                value=5,\n",
    "                step=1,\n",
    "                label=\"Number of Messages for Context\"\n",
    "            )\n",
    "            show_retrieved_cb = gr.Checkbox(\n",
    "                label=\"Show Retrieved Context\",\n",
    "                value=False\n",
    "            )\n",
    "            qa_btn = gr.Button(\"Get Answer\")\n",
    "            qa_output = gr.Textbox(label=\"Answer\", lines=20)\n",
    "            \n",
    "            qa_btn.click(\n",
    "                fn=answer_question,\n",
    "                inputs=[\n",
    "                    question_input,\n",
    "                    qa_channel_filter,\n",
    "                    temperature_slider,\n",
    "                    context_slider,\n",
    "                    show_retrieved_cb\n",
    "                ],\n",
    "                outputs=qa_output\n",
    "            )\n",
    "    \n",
    "    # Launch the Gradio app\n",
    "    try:\n",
    "        demo.queue().launch(debug=True)\n",
    "    except Exception:\n",
    "        demo.queue().launch(share=True, debug=True)\n",
    "\n",
    "# Uncomment the next line to launch the Gradio UI\n",
    "# launch_gradio_app()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've created a comprehensive RAG system for Telegram messages using OpenVINO-optimized models. The system includes:\n",
    "\n",
    "1. **Model Selection and Conversion**: We selected and converted models for embedding, reranking, and LLM generation, with options for different language support and model compression (INT4/INT8/FP16).\n",
    "\n",
    "2. **Device Optimization**: We configured models to run on optimal hardware (CPU/GPU/NPU) based on availability.\n",
    "\n",
    "3. **Telegram Integration**: We implemented Telegram channel message downloading and article extraction.\n",
    "\n",
    "4. **RAG Pipeline**: We built a complete RAG pipeline with embedding, vector storage, retrieval, optional reranking, and generation.\n",
    "\n",
    "5. **Query Interface**: We created interfaces for both simple searches and question answering using the RAG system.\n",
    "\n",
    "This notebook demonstrates how to apply the same techniques from the general RAG notebook to a specific data source (Telegram), showing the flexibility and power of combining OpenVINO-optimized models with the LangChain framework.\n",
    "\n",
    "For further improvements, you could:\n",
    "\n",
    "- Add streaming responses to the interface\n",
    "- Implement periodic data updates\n",
    "- Add more filtering options (date ranges, message types, etc.)\n",
    "- Explore different embedding and reranking models for improved retrieval quality\n",
    "- Implement performance benchmarking to compare different model compression techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Telegram Channel Integration with RAG System\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Set up the Telegram client\n",
    "2. Download messages from specified channels\n",
    "3. Process these messages into a vector store\n",
    "4. Query the processed messages using RAG\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we need to get your Telegram API credentials:\n",
    "1. Go to https://my.telegram.org/apps\n",
    "2. Create a new application\n",
    "3. Note down `api_id` and `api_hash`\n",
    "\n",
    "Create a `.env` file in this directory with your credentials:\n",
    "```\n",
    "TELEGRAM_API_ID=your_api_id\n",
    "TELEGRAM_API_HASH=your_api_hash\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements_telegram.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from telegram_ingestion import TelegramChannelIngestion\n",
    "from telegram_rag_integration import TelegramRAGIntegration\n",
    "import asyncio\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 1: Download Messages from Telegram Channels\n",
    "\n",
    "Specify channels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 30 messages from 1 channels\n"
     ]
    }
   ],
   "source": [
    "# List channels here\n",
    "channels = [\"guardian\"]\n",
    "\n",
    "async def download_messages():\n",
    "    ingestion = TelegramChannelIngestion(\n",
    "        api_id=os.getenv(\"TELEGRAM_API_ID\"),\n",
    "        api_hash=os.getenv(\"TELEGRAM_API_HASH\")\n",
    "    )\n",
    "    \n",
    "    await ingestion.start()\n",
    "    try:\n",
    "        messages = await ingestion.process_channels(\n",
    "            channels,\n",
    "            limit_per_channel=30,  # Can be changed\n",
    "            since_hours=24  # Can be changed\n",
    "        )\n",
    "        print(f\"Downloaded {len(messages)} messages from {len(channels)} channels\")\n",
    "    finally:\n",
    "        await ingestion.stop()\n",
    "\n",
    "await download_messages()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 2: Process Messages into Vector Store\n",
    "\n",
    "Process the downloaded messages and add them to RAG system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Question Answering with RAG\n",
    "\n",
    "Now let's use the RAG system to answer questions about the Telegram messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genai_helper import ChunkStreamer\n",
    "from ov_langchain_helper import OpenVINOLLM\n",
    "import openvino as ov\n",
    "\n",
    "# Initialize the LLM\n",
    "model_id = \"qwen2.5-3b-instruct/INT4_compressed_weights\"  # You can change this to any supported model\n",
    "llm = OpenVINOLLM.from_model_path(\n",
    "    model_path=model_id,\n",
    "    device=\"CPU\"\n",
    ")\n",
    "\n",
    "# Example questions\n",
    "questions = [\n",
    "    \"What are the main topics discussed in the Bloomberg channel?\",\n",
    "    \"What are the latest updates from The Guardian?\",\n",
    "    \"Are there any discussions about technology or AI?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    \n",
    "    # Update LLM configuration\n",
    "    llm.config.temperature = 0.7\n",
    "    llm.config.top_p = 0.9\n",
    "    llm.config.top_k = 50\n",
    "    llm.config.repetition_penalty = 1.1\n",
    "    \n",
    "    answer = rag.answer_question(\n",
    "        question=question,\n",
    "        llm=llm,\n",
    "        k=5  # Number of relevant messages to retrieve\n",
    "    )\n",
    "    print(f\"Answer: {answer}\\n\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Channel-Specific Questions\n",
    "\n",
    "We can also ask questions about specific channels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example channel-specific questions\n",
    "channel_questions = [\n",
    "    (\"bloomberg\", \"What are the latest economic updates?\"),\n",
    "    (\"guardian\", \"What are the main political stories?\")\n",
    "]\n",
    "\n",
    "for channel, question in channel_questions:\n",
    "    print(f\"\\nChannel: {channel}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    \n",
    "    # Update LLM configuration\n",
    "    llm.config.temperature = 0.7\n",
    "    llm.config.top_p = 0.9\n",
    "    llm.config.top_k = 50\n",
    "    llm.config.repetition_penalty = 1.1\n",
    "    \n",
    "    answer = rag.answer_question(\n",
    "        question=question,\n",
    "        llm=llm,\n",
    "        k=5,\n",
    "        filter_dict={\"channel\": channel}\n",
    "    )\n",
    "    print(f\"Answer: {answer}\\n\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = TelegramRAGIntegration(\n",
    "    embedding_model_name=\"BAAI/bge-small-en-v1.5\",  # Can be changed\n",
    "    vector_store_path=\"telegram_vector_store\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "rag.process_telegram_data_dir()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 3: Query the Processed Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the latest announcements?\n",
      "\n",
      "Result 1:\n",
      "Channel: bloomberg\n",
      "Date: 2025-05-04T07:57:30+00:00\n",
      "Content: 🎙 Trump wasn't on the ballot in Australia and Singapore elections, but his tariffs and policies loomed large over the results.\n",
      "\n",
      "Bloomberg reporters take your questions on what's next - tune in on Mond...\n",
      "\n",
      "Result 2:\n",
      "Channel: bloomberg\n",
      "Date: 2025-05-04T07:57:30+00:00\n",
      "Content: 🎙 Trump wasn't on the ballot in Australia and Singapore elections, but his tariffs and policies loomed large over the results.\n",
      "\n",
      "Bloomberg reporters take your questions on what's next - tune in on Mond...\n",
      "\n",
      "Result 3:\n",
      "Channel: bloomberg\n",
      "Date: 2025-05-04T07:57:30+00:00\n",
      "Content: 🎙 Trump wasn't on the ballot in Australia and Singapore elections, but his tariffs and policies loomed large over the results.\n",
      "\n",
      "Bloomberg reporters take your questions on what's next - tune in on Mond...\n",
      "\n",
      "Result 4:\n",
      "Channel: bloomberg\n",
      "Date: 2025-05-04T07:57:30+00:00\n",
      "Content: 🎙 Trump wasn't on the ballot in Australia and Singapore elections, but his tariffs and policies loomed large over the results.\n",
      "\n",
      "Bloomberg reporters take your questions on what's next - tune in on Mond...\n",
      "\n",
      "Result 5:\n",
      "Channel: bloomberg\n",
      "Date: 2025-05-04T07:57:30+00:00\n",
      "Content: 🎙 Trump wasn't on the ballot in Australia and Singapore elections, but his tariffs and policies loomed large over the results.\n",
      "\n",
      "Bloomberg reporters take your questions on what's next - tune in on Mond...\n",
      "\n",
      "Query: Any updates about new features?\n",
      "\n",
      "Result 1:\n",
      "Channel: guardian\n",
      "Date: 2025-05-27T05:53:56+00:00\n",
      "Content: [**Tuesday briefing: Trump’s statements about Putin have changed. Will his actions catch up?**](https://www.theguardian.com/world/2025/may/27/tuesday-briefing-first-edition-trump-putin-relationship?CM...\n",
      "\n",
      "Result 2:\n",
      "Channel: guardian\n",
      "Date: 2025-05-27T05:53:56+00:00\n",
      "Content: [**Tuesday briefing: Trump’s statements about Putin have changed. Will his actions catch up?**](https://www.theguardian.com/world/2025/may/27/tuesday-briefing-first-edition-trump-putin-relationship?CM...\n",
      "\n",
      "Result 3:\n",
      "Channel: guardian\n",
      "Date: 2025-05-27T05:53:56+00:00\n",
      "Content: [**Tuesday briefing: Trump’s statements about Putin have changed. Will his actions catch up?**](https://www.theguardian.com/world/2025/may/27/tuesday-briefing-first-edition-trump-putin-relationship?CM...\n",
      "\n",
      "Result 4:\n",
      "Channel: guardian\n",
      "Date: 2025-05-27T05:53:56+00:00\n",
      "Content: [**Tuesday briefing: Trump’s statements about Putin have changed. Will his actions catch up?**](https://www.theguardian.com/world/2025/may/27/tuesday-briefing-first-edition-trump-putin-relationship?CM...\n",
      "\n",
      "Result 5:\n",
      "Channel: guardian\n",
      "Date: 2025-05-27T05:53:56+00:00\n",
      "Content: [**Tuesday briefing: Trump’s statements about Putin have changed. Will his actions catch up?**](https://www.theguardian.com/world/2025/may/27/tuesday-briefing-first-edition-trump-putin-relationship?CM...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def query_messages(query: str, k: int = 5):\n",
    "    results = rag.query_messages(query, k=k)\n",
    "    \n",
    "    print(f\"Query: {query}\\n\")\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"Result {i}:\")\n",
    "        print(f\"Channel: {doc.metadata['channel']}\")\n",
    "        print(f\"Date: {doc.metadata['date']}\")\n",
    "        print(f\"Content: {doc.page_content[:200]}...\\n\")\n",
    "\n",
    "query_messages(\"What are the latest announcements?\")\n",
    "query_messages(\"Any updates about new features?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 4: Filter by Channel\n",
    "\n",
    "We can filter results to specific channels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from bloomberg:\n",
      "\n",
      "Date: 2025-04-30T04:08:47+00:00\n",
      "Content: 🎙 LIVE NOW: Can Australia's next government fix its economy?\n",
      "\n",
      "Ahead of the country's federal election on Saturday, Bloomberg reporters are taking your questions on the main parties' plans in a Live Q&...\n",
      "\n",
      "Date: 2025-04-30T04:08:47+00:00\n",
      "Content: 🎙 LIVE NOW: Can Australia's next government fix its economy?\n",
      "\n",
      "Ahead of the country's federal election on Saturday, Bloomberg reporters are taking your questions on the main parties' plans in a Live Q&...\n",
      "\n",
      "Date: 2025-04-30T04:08:47+00:00\n",
      "Content: 🎙 LIVE NOW: Can Australia's next government fix its economy?\n",
      "\n",
      "Ahead of the country's federal election on Saturday, Bloomberg reporters are taking your questions on the main parties' plans in a Live Q&...\n",
      "\n",
      "Date: 2025-04-30T04:08:47+00:00\n",
      "Content: 🎙 LIVE NOW: Can Australia's next government fix its economy?\n",
      "\n",
      "Ahead of the country's federal election on Saturday, Bloomberg reporters are taking your questions on the main parties' plans in a Live Q&...\n",
      "\n",
      "Date: 2025-04-30T04:08:47+00:00\n",
      "Content: 🎙 LIVE NOW: Can Australia's next government fix its economy?\n",
      "\n",
      "Ahead of the country's federal election on Saturday, Bloomberg reporters are taking your questions on the main parties' plans in a Live Q&...\n"
     ]
    }
   ],
   "source": [
    "specific_channel = \"bloomberg\"\n",
    "results = rag.query_messages(\n",
    "    \"What are the latest updates?\",\n",
    "    k=5,\n",
    "    filter_dict={\"channel\": specific_channel}\n",
    ")\n",
    "\n",
    "print(f\"Results from {specific_channel}:\")\n",
    "for doc in results:\n",
    "    print(f\"\\nDate: {doc.metadata['date']}\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
